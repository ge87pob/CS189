{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would highly recommend looking at `neural_networks.grad_check.check_gradients` and making sure you understand how numerical gradient checking is being carried out. This function is used in the notebook to check the gradients of the neural network layers you write. Make sure to check the gradient of a layer after finishing its implementation.\n",
    "\n",
    "The function returns the relative error of the numerical gradient (approximated using finite differences) with respect to the analytical gradient (computed via backpropagation). Correct implementations should get very small errors, usually less than `1e-8` for 64-bit float matrices (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from neural_networks.utils import check_gradients\n",
    "from neural_networks.layers import FullyConnected\n",
    "from neural_networks.activations import Linear, ReLU, SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for linear activation: 2.559352689324099e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "linear_activation = Linear()\n",
    "_ = linear_activation.forward(X)\n",
    "grad = linear_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for linear activation:\",\n",
    "    check_gradients(\n",
    "        fn=linear_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for relu activation: 4.113328779137742e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "relu_activation = ReLU()\n",
    "out = relu_activation.forward(X)\n",
    "grad = relu_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for relu activation:\",\n",
    "    check_gradients(\n",
    "        fn=relu_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m grad \u001b[38;5;241m=\u001b[39m softmax_activation\u001b[38;5;241m.\u001b[39mbackward(X, dLdY)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# check the gradients w.r.t. each parameter\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelative error for softmax activation:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 13\u001b[0m     check_gradients(\n\u001b[1;32m     14\u001b[0m         fn\u001b[38;5;241m=\u001b[39msoftmax_activation\u001b[38;5;241m.\u001b[39mforward,  \u001b[38;5;66;03m# the function we are checking\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         grad\u001b[38;5;241m=\u001b[39mgrad,  \u001b[38;5;66;03m# the analytically computed gradient\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         x\u001b[38;5;241m=\u001b[39mX,        \u001b[38;5;66;03m# the variable w.r.t. which we are taking the gradient\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         dLdf\u001b[38;5;241m=\u001b[39mdLdY,  \u001b[38;5;66;03m# gradient at previous layer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/VSC/CS189/hw3/code/neural_networks/utils.py:90\u001b[0m, in \u001b[0;36mcheck_gradients\u001b[0;34m(fn, grad, x, dLdf, h)\u001b[0m\n\u001b[1;32m     88\u001b[0m oldval \u001b[38;5;241m=\u001b[39m x[ix]\n\u001b[1;32m     89\u001b[0m x[ix] \u001b[38;5;241m=\u001b[39m oldval \u001b[38;5;241m+\u001b[39m h\n\u001b[0;32m---> 90\u001b[0m pos \u001b[38;5;241m=\u001b[39m fn(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     91\u001b[0m x[ix] \u001b[38;5;241m=\u001b[39m oldval \u001b[38;5;241m-\u001b[39m h\n\u001b[1;32m     92\u001b[0m neg \u001b[38;5;241m=\u001b[39m fn(x)\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 3)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "softmax_activation = SoftMax()\n",
    "_ = softmax_activation.forward(X)\n",
    "grad = softmax_activation.backward(X, dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "print(\n",
    "    f\"Relative error for softmax activation:\",\n",
    "    check_gradients(\n",
    "        fn=softmax_activation.forward,  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=dLdY,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checks for Full Layers (Linear Activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XavierUniform.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# initialize a fully connected layer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# and perform a forward and backward pass\u001b[39;00m\n\u001b[1;32m      6\u001b[0m fc_layer \u001b[38;5;241m=\u001b[39m FullyConnected(n_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m _ \u001b[38;5;241m=\u001b[39m fc_layer\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m      8\u001b[0m _ \u001b[38;5;241m=\u001b[39m fc_layer\u001b[38;5;241m.\u001b[39mbackward(dLdY)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# check the gradients w.r.t. each parameter\u001b[39;00m\n",
      "File \u001b[0;32m~/VSC/CS189/hw3/code/neural_networks/layers.py:137\u001b[0m, in \u001b[0;36mFullyConnected.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# initialize layer parameters if they have not been initialized\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_parameters(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR CODE ###\u001b[39;00m\n\u001b[1;32m    141\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/VSC/CS189/hw3/code/neural_networks/layers.py:111\u001b[0m, in \u001b[0;36mFullyConnected._init_parameters\u001b[0;34m(self, X_shape)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_in \u001b[38;5;241m=\u001b[39m X_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m### BEGIN YOUR CODE ###\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_in, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_out)\n\u001b[1;32m    112\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_out))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m OrderedDict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m: W, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: b}) \u001b[38;5;66;03m# DO NOT CHANGE THE KEYS\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: XavierUniform.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"linear\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
